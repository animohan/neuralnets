---
title: "Backpropagation"
---

## Exploring the example given in the AI-WorkShop by James McCaffrey.

### Sample Feed Forward Neural Network

#### Part 1: 
Goal here is to find the change in weights to reduce the output error. In the first section, we will look at how the weights to the output layer can be modified based on the output values and the error rate. 

I have added one more node to explicitly show the Softmax operations as it is easier to show calculations.


![](NN1.PNG)


1) Lets look at the output from the last hidden layer:
    $$u = (x_{0}*w_{01}) + (x_{1}*w_{11}) + (x_{2}*w_{21}) + (x_{3}*w_{31}) + (x_{4}*w_{41})$$

2) Now, lets apply the softmax function to the output $u$ to get the output $o_{1}$
    $$o_{1} = \frac{e^{u}}{e^u + e^v}$$
    
    Similarlty, if we assume the output from the 2nd output node is $v$ then we have:
    $$o_{2} = \frac{e^{v}}{e^u + e^v}$$

    
3) As James showed, the error is specified as:
    $$E = \frac{1}{2} * \sum_{j = 1}^2 (t_{j} - o_{j})^{2}$$
    

4) Now, our goal is to calculate the change in weight that can reduce the error e.g if we consider $w_{11}$ as the 
  weight under consideration we want to calculate: $\frac{\partial E}{\partial w_{11}}$

5) For calculating this, we use the chain rule:
  
  $$ \frac{\partial E}{\partial W_{11}} = \frac{\partial E}{\partial o_{1}}* \frac{\partial o_{1}}{\partial u} *  \frac{\partial u}{\partial W_{11}} $$

6) Now lets calculate:$\frac{\partial E}{\partial o_{1}}$:
    
    $\frac{\partial E}{\partial o_{1}}  = 2*\frac{1}{2}* (t_{1}-o_{1}) * \frac{\partial {(t_{1}-o_{1})}}{\partial o_{1}}$
          
    $\frac{\partial E}{\partial o_{1}}  =  -1 * (t_{1}-o_{1})$
        
    $\frac{\partial E}{\partial o_{1}}  =  (o_{1}-t_{1})$ 
  
  
7) Next we have to calculate: $\frac{\partial o_{1}}{\partial u}$
    
    Remember $o_{1} = \frac{e^{u}}{e^u + e^v}$
    
    $\frac{\partial o_{1}}{\partial u} = \frac{\partial {\frac{e^{u}}{e^u + e^v} }}{\partial u}$
    
    $\frac{\partial o_{1}}{\partial u} = \frac{(e^u + e^v)*e^u - e^u*e^u}{(e^u+e^v)^2}$
    
    $\frac{\partial o_{1}}{\partial u} = \frac{e^u * e^v}{(e^u + e^v)^2}$
    
    $\frac{\partial o_{1}}{\partial u} = \frac{e^u}{e^u + e^v} * \frac{e^v}{e^u + e^v}`$
    
    $\frac{\partial o_{1}}{\partial u} = \frac{e^u}{e^u + e^v} * (1 - \frac{e^u}{e^u + e^v} )$
    
    $\frac{\partial o_{1}}{\partial o_1} = o_1 * (1- o_1)$

8) Lastly, we calculate: $\frac{\partial u}{\partial w_{11}}$ 
    
    Remember
      $u = (x_{0}*w_{01}) + (x_{1}*w_{11}) + (x_{2}*w_{21}) + (x_{3}*w_{31}) + (x_{4}*w_{41})$
    
    
    $\frac{\partial u}{\partial w_{11}} = \frac{\partial (x_{0}*w_{01}) + (x_{1}*w_{11}) + (x_{2}*w_{21}) + (x_{3}*w_{31}) + (x_{4}*w_{41})  }{\partial w_{11}}$
    
    $\frac{\partial u}{\partial w_{11}} = x_1$


9) Now combining equations from (5),(6),(7) and (8):


    $\frac{\partial E}{\partial w_{11}} = \frac{\partial E}{\partial o_{1}}* \frac{\partial o_{1}}{\partial u}* \frac{\partial u}{\partial w_{11}}$
    
    
    $\frac{\partial E}{\partial w_{11}} = (o_{1}-t_{1}) * o_1 * (1- o_1) * x_1$
      
    Rearranging terms:   
      
    $\frac{\partial E}{\partial w_{11}} = x_1 * (o_{1}-t_{1}) * o_1 * (1- o_1)$
  
10) And similarly we can calculate the change in Error based on another weight
    
    $\frac{\partial E}{\partial w_{21}}$ = $x_2 * (o_{1}-t_{1})$ $*$ $o_1 * (1- o_1)$
    
    or in general:
    
    $\frac{\partial E}{\partial w_{ij}}$ = $x_i * (o_{j}-t_{j})$ $*$ $o_j * (1- o_j)$
    

11) We can now use this error to update the weights:

    $w_{ij} = w_{ij} + \alpha * \frac{\partial E}{\partial w_{ij}}$

    where $\alpha$ is the learning rate



#### Part 2: 
Goal  for Part 2 is to find the change in weights in the hidden layer to reduce the output error. Following the backpropgation algorithm that James showed in the class.

1) Let us take a particular hidden node :- h1
   This node has input from the following nodes $x_{01}, x_{02} and x_{03}$ and the weights of the inputs are $w_{01},     w_{02}, w_{03}$. $w_{00}$ is the bias term.
   
      For convenience let us say:
      $ u = (x_{00} * w_{00}) + (x_{01} * w_{01}) + (x_{02} * w_{02}) + (x_{03} * w_{03})$
    
     Additionally, we are using the tanh function for non-linear thresholding. So let us say:
       $h_1 = tanh(u)$ is the output of the hidden node.
    
2) Now to find the error propogated to the input weigth to the hidden node h1 = w11

    $\frac{\partial E}{\partial w_{11}}$  =  Sum of errors propagated to node h1 * Derivative of output of h1 w.r.t to the input weight w11
   
    $\frac{\partial E}{\partial w_{11}} =  \frac{\partial E}{\partial h_1} * \frac{\partial h_1}{\partial u} * \frac{\partial u}{\partial w_{11}}$
    
    Let us calculate each term individually
    
3) $\frac{\partial u}{\partial w_{11}}$
  
      $\frac{\partial u}{\partial w_{11}}$ = $\frac{\partial (x_{0}*w_{01}) + (x_{1}*w_{11}) + (x_{2}*w_{21}) + (x_{3}*w_{31}) }{\partial w_{11}}$
      
      $\frac{\partial u}{\partial w_{11}}$ = $x_{1}$
      
      similarly,
      
      $\frac{\partial u}{\partial w_{21}}$ = $x_{2}$
      
      And in general
      
      $\frac{\partial u}{\partial w_{i1}}$ = $x_{i}$
  
4) Note: $h_1 = tanh(u)$ is the output of the hidden node.
                
    $\frac{\partial h_{1}}{\partial u}$ = $\frac{\partial tanh(u)}{\partial u}$
                
      $=\frac{\partial \frac{sinh(u)}{cosh(u)}}{\partial u}$
                
      $=\frac{cosh(u)* \frac{dsinh(u)}{du} - sinh(u) * \frac{dcosh(u)}{du}  }{(cosh(u))^2}$
                
      $=\frac{(cosh(u))^2 - (sinh(u))^2}{(cosh(u))^2}$
                
      $= 1 - (tanh(u))^2$
                
      $= (1-tanh(u))(1 + tanh(u))$
                
      $\frac{\partial h_{1}}{\partial u}$ $= (1-h_1)(1+h_1)$
          
      In general,
      
      $\frac{\partial h_{j}}{\partial u}$ $= (1-h_j)(1+h_j)$
      
5) Lastly, we have to calculate:
  $\frac{\partial E}{\partial h_1}$  = Backpropogated error at the output of h1
  
#### How is $\frac{\partial E}{\partial h_1}$  = $\sum \delta_j*w_j$ ?
           
           
        
